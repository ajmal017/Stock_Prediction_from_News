{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get open, high, low and close data from Yahoo Finance\n",
    "## We will be getting atleast 5 years historical prices for S&P 500 stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the required libraries\n",
    "We will be using yfinance API adn Yahoo Query to pull data from Yahoo Finance. To install this library use pip install yfinance, pip install yahooquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import yahooquery as yq\n",
    "import pandas as pd\n",
    "import csv\n",
    "import logging #library to create log files\n",
    "from datetime import datetime # To get the current date and time\n",
    "from datetime import date\n",
    "from pytz import timezone # Get timezone\n",
    "import csv\n",
    "import time\n",
    "import os # To check if the file exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load tickers for S&P 500 stocks from csv file \n",
    "Note: We have created seperate python script (S&P500Tickers.ipynb) to get S&P 500 stocks and store that in file name is 'S&P500Tickers.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a function that pull data for yfinance api for each ticker\n",
    "OHLC - Open high low and close of a stock on a given date\n",
    "Script needs start and end date to pull data between the range else data for max period will be pulled\n",
    "Start date and end date has to be in format YYYY-MM-DD\n",
    "Add daily percent return [(close - Open) / Open]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OHLC_data(tickers, days=\"max\"):\n",
    "    '''\n",
    "    This function gets daily OHLC data from Yahoo Finance API for the provided number of days. \n",
    "    Daily returns are caluclated and stored in Returns column\n",
    "    In case days is not provided then data sis pulled for maximum number of days\n",
    "    \n",
    "    Input Parameters: \n",
    "    tickers: Ticker object from yahoo group for each of the S&P 500 symbols\n",
    "    days: Number of days for which data needs to be pulled\n",
    "    \n",
    "    Returns: Dataframe of the extracted data\n",
    "    '''\n",
    "    logging.info(\"Getting OHLC data for days provided\")\n",
    "    OHLC_data = yfAPI.download(tickers = tickers_list, period=days, interval = \"1d\", group_by = 'ticker', \n",
    "                               auto_adjust = True, prepost = False, threads = True, proxy = None) \n",
    "    \n",
    "    # Add daily percent change on the price whic is [(close price - open price)/open price]\n",
    "    #OHLC_data['Return'] = (OHLC_data['Close'] - OHLC_data['Open'])/OHLC_data['Open']\n",
    "    return OHLC_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get the last date and dataframe from the existing OHLC csv file\n",
    "This function will open existing csv file and put the data in existing dataframe and will also return last date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_date_data(file):\n",
    "    '''\n",
    "    This function open existing OHLC file, put that in pandas dataframe and extract the last date from the file\n",
    "    Input Parameters: Name of the existing csv file that needs to be read in dataframe\n",
    "    Returns: Last date and pandas dataframe containing data\n",
    "    '''\n",
    "    logging.info(\"Opening existing data file and extracting last date from it\")\n",
    "    \n",
    "    old_data_df = pd.read_csv(file) # Read the csv file in a dataframe\n",
    "    \n",
    "    # Convert Data column to datetime\n",
    "    old_data_df['Date']= pd.to_datetime(old_data_df['Date'])\n",
    "    \n",
    "    previous_date = max(old_data_df['Date']) # Get the maximum date which is the last date for which data is present\n",
    "    print(f'Last date is {previous_date}')\n",
    "    \n",
    "    logging.info(f'Last date in the file is {previous_date}')\n",
    "    \n",
    "    return previous_date, old_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main function to run all the sub functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    tickers_list = ['AAPL', 'MSFT']\n",
    "    \n",
    "    # Get the current date in YYYY-MM-DD format. This will be end date as well\n",
    "    #tz = timezone('US/Eastern') # Get in estern timzezone\n",
    "    #current_date = datetime.now(tz).strftime('%Y-%m-%d')\n",
    "    today = date.today()\n",
    "    \n",
    "    # Create name of the log file\n",
    "    log_file_name = 'OHLC_logfile_' + str(today) +'.log'\n",
    "    \n",
    "    # Initialize a log file at the Info level. This is just to ensure smooth debugging in case anything fails\n",
    "    # %(asctime)s adds the time of creation of the LogRecord\n",
    "    logging.basicConfig(filename=log_file_name, filemode=\"w\", format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "    \n",
    "    logging.info(\"In the main function\")\n",
    "    logging.info(f'Processing for date {today}')\n",
    "    \n",
    "    print(today)\n",
    "    print(log_file_name)\n",
    "    \n",
    "    # Name of the output file\n",
    "    OHLC_data_file = \"OHLC_data.csv\"\n",
    "    \n",
    "    logging.info(\"Check if file already exists\")\n",
    "    \n",
    "    # Check if the OHLC csv file existing the root folder from where this script is ran\n",
    "    if os.path.isfile(OHLC_data_file):\n",
    "        logging.info(\"OHLC Data file exists. Getting data in dataframe and last date\")\n",
    "        # Get the last date and dataframe\n",
    "        previous_date, old_data_df = get_last_date_data(OHLC_data_file)\n",
    "        \n",
    "        # Get difference between last date and todays date in days. This will be passed to OHLC function\n",
    "        no_days = str((today-previous_date).days)+'d' # Days to be passed in OHLC function is in format \"1d\"\n",
    "        latest_data = get_OHLC_data(tickers_list, no_days)\n",
    "        # Merge new data with old data and avoid duplicates\n",
    "        final_df = old_data_df.append(latest_data).drop_duplicates()\n",
    "    else:\n",
    "        logging.info(\"OHLC file does not exists. Getting maximum possible data\")\n",
    "        latest_data = get_OHLC_data(tickers_list) # Get the max data\n",
    "        final_df = latest_data\n",
    "    \n",
    "    # Create name of the output file\n",
    "    logging.info(f'Writing data in the file name {OHLC_data_file}')\n",
    "    final_df.to_csv(OHLC_data_file, mode='w', index=True) #index is True as we want to write index in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_date = str(datetime.today()).split()[0]\n",
    "#print(current_date)\n",
    "#datetime.today()\n",
    "tz = timezone('US/Eastern')\n",
    "x = datetime.now(tz).strftime('%Y-%m-%d')\n",
    "print(x)\n",
    "#datetime.now(tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_obj = datetime.strptime(\"2020-10-19\", '%Y-%m-%d')\n",
    "y = datetime_obj.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "print(today)\n",
    "delta = str((today-y).days)+'d'\n",
    "print(delta)\n",
    "log_file_name = 'OHLC_logfile_' + str(today) +'.log'\n",
    "print(log_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(tickers = \"AAPL MSFT\", \n",
    "        \n",
    "        #start=\"2020-10-19\", end=\"2020-10-22\",\n",
    "\n",
    "        # use \"period\" instead of start/end\n",
    "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "        # (optional, default is '1mo')\n",
    "         period = \"3d\",\n",
    "\n",
    "        # fetch data by interval (including intraday if period < 60 days)\n",
    "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        # (optional, default is '1d')\n",
    "        interval = \"1d\",\n",
    "\n",
    "        # group by ticker (to access via data['SPY'])\n",
    "        # (optional, default is 'column')\n",
    "        group_by = 'ticker',\n",
    "\n",
    "        # adjust all OHLC automatically\n",
    "        # (optional, default is False)\n",
    "        auto_adjust = True,\n",
    "\n",
    "        # download pre/post regular market hours data\n",
    "        # (optional, default is False)\n",
    "        prepost = False,\n",
    "\n",
    "        # use threads for mass downloading? (True/False/Integer)\n",
    "        # (optional, default is True)\n",
    "        threads = True,\n",
    "\n",
    "        # proxy URL scheme use use when downloading?\n",
    "        # (optional, default is None)\n",
    "        proxy = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = yf.download(tickers = \"AAPL MSFT\", \n",
    "        \n",
    "        #start=\"2020-10-19\", end=\"2020-10-22\",\n",
    "\n",
    "        # use \"period\" instead of start/end\n",
    "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "        # (optional, default is '1mo')\n",
    "         period = \"1d\",\n",
    "\n",
    "        # fetch data by interval (including intraday if period < 60 days)\n",
    "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        # (optional, default is '1d')\n",
    "        interval = \"1d\",\n",
    "\n",
    "        # group by ticker (to access via data['SPY'])\n",
    "        # (optional, default is 'column')\n",
    "        group_by = 'ticker',\n",
    "\n",
    "        # adjust all OHLC automatically\n",
    "        # (optional, default is False)\n",
    "        auto_adjust = True,\n",
    "\n",
    "        # download pre/post regular market hours data\n",
    "        # (optional, default is False)\n",
    "        prepost = False,\n",
    "\n",
    "        # use threads for mass downloading? (True/False/Integer)\n",
    "        # (optional, default is True)\n",
    "        threads = True,\n",
    "\n",
    "        # proxy URL scheme use use when downloading?\n",
    "        # (optional, default is None)\n",
    "        proxy = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.unstack()\n",
    "display(data)\n",
    "display(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = data.append(data1).drop_duplicates()\n",
    "display(data)\n",
    "final_df.to_csv(\"test.csv\", mode='w') #index is False as we don't want to write index in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahooquery import Ticker\n",
    "\n",
    "faang = Ticker('fb', asynchronous = True, formatted=False, \n",
    "             max_workers = 8, proxies = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = faang.history(period = \"7d\", interval =\"1m\")\n",
    "x.to_csv(\"test.csv\", mode='w') #index is False as we don't want to write index in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__module__\n",
      "__doc__\n",
      "__init__\n",
      "_quote_summary\n",
      "_quote_summary_dataframe\n",
      "_to_dataframe\n",
      "all_modules\n",
      "get_modules\n",
      "asset_profile\n",
      "calendar_events\n",
      "earnings\n",
      "earnings_trend\n",
      "esg_scores\n",
      "financial_data\n",
      "news\n",
      "index_trend\n",
      "industry_trend\n",
      "key_stats\n",
      "major_holders\n",
      "page_views\n",
      "price\n",
      "quote_type\n",
      "quotes\n",
      "recommendations\n",
      "share_purchase_activity\n",
      "summary_detail\n",
      "summary_profile\n",
      "technical_insights\n",
      "_financials\n",
      "_financials_dataframes\n",
      "all_financial_data\n",
      "get_financial_data\n",
      "corporate_events\n",
      "corporate_guidance\n",
      "valuation_measures\n",
      "balance_sheet\n",
      "cash_flow\n",
      "company_officers\n",
      "earning_history\n",
      "fund_ownership\n",
      "grading_history\n",
      "income_statement\n",
      "insider_holders\n",
      "insider_transactions\n",
      "institution_ownership\n",
      "recommendation_trend\n",
      "sec_filings\n",
      "_fund_holdings\n",
      "fund_bond_holdings\n",
      "fund_category_holdings\n",
      "fund_equity_holdings\n",
      "fund_performance\n",
      "fund_profile\n",
      "fund_holding_info\n",
      "fund_top_holdings\n",
      "fund_bond_ratings\n",
      "fund_sector_weightings\n",
      "p_all_financial_data\n",
      "p_get_financial_data\n",
      "p_balance_sheet\n",
      "p_cash_flow\n",
      "p_corporate_events\n",
      "p_income_statement\n",
      "p_company_360\n",
      "p_technical_insights\n",
      "p_portal\n",
      "p_reports\n",
      "p_ideas\n",
      "p_technical_events\n",
      "p_valuation_measures\n",
      "p_value_analyzer\n",
      "p_value_analyzer_drilldown\n",
      "history\n",
      "_history_1m\n",
      "_historical_data_to_dataframe\n",
      "_adjust_ohlc\n",
      "option_chain\n",
      "_option_dataframe\n"
     ]
    }
   ],
   "source": [
    "for i in Ticker.__dict__.keys() :\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
