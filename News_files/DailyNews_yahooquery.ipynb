{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get daily news form using Yahoo Query API\n",
    "## We will be getting all the available historical news from Yahoo Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the required libraries\n",
    "We will be using Yahoo Query API to pull data from Yahoo Finance. To install this library use pip install yahooquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yahooquery as yq\n",
    "import pandas as pd\n",
    "import csv\n",
    "import logging #library to create log files\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "# from pytz import timezone # Get timezone\n",
    "import time\n",
    "import os # To check if the file exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load tickers for S&P 500 stocks from csv file \n",
    "Note: We have created seperate python script (S&P500Tickers.ipynb) to get S&P 500 stocks and store that in file name is 'S&P500Tickers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_data = pd.read_csv(\"S&P500Tickers.csv\", encoding='utf-8') # Open csv file and read data in pandas dataframe\n",
    "tickers_list = ticker_data['Symbol'].tolist() #Get all the tickers in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a function that pull news from yahooquery api for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_news(tickers, count = 500): # Maixum number of news items per ticker\n",
    "    '''\n",
    "    This function pull news data from yahoo query API. The number of news items pulled depends in the input count \n",
    "        \n",
    "    Input Parameters: \n",
    "    tickers: List of tickers for which data needs to be extracted\n",
    "    Count: number of news articles that needs to be extracted\n",
    "    \n",
    "    Returns: Dataframe of the extracted data that includes Symbol, Date, Headline, Summary, Source and link\n",
    "    '''\n",
    "    logging.info(\"Pulling daily news\")\n",
    "    final_news_df = pd.DataFrame() # Declearing final data frame\n",
    "    news_data = pd.DataFrame() # Declearing intermediate data frame\n",
    "    for ticker in tickers:\n",
    "        # Initiate object to get the data from API\n",
    "        #print(f'working on the ticker {ticker}')\n",
    "        try:\n",
    "            ticker_obj = yq.Ticker(ticker) # Create ticker onject\n",
    "            news_list = ticker_obj.news(count) # Get news. Returned as list\n",
    "            #news_data = pd.DataFrame(news_list) # Creating pandas data frame from list of dictionaries\n",
    "            # Creating pandas data frame from list of dictionaries, with specified columns\n",
    "            news_data = pd.DataFrame(news_list, columns=['provider_publish_time', 'id', 'title']) # Creating pandas data frame from list of dictionaries\n",
    "            news_data.insert(0, 'Symbol', ticker) #Adding ticker to the dataframe\n",
    "\n",
    "            #display(news_data.head())\n",
    "            #print(f'Appending to existing dataframe')\n",
    "            final_news_df = final_news_df.append(news_data) # Appending to dataframe that will contain news for all the tickers\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        #display(final_news_df.head())\n",
    "        time.sleep(.5) # Speeed time between each data request\n",
    "    \n",
    "    #final_news_df.reset_index(inplace=True) #Reset the index\n",
    "    #display(final_news_df.head())\n",
    "    # Convert datetime from epoch to datetime\n",
    "    final_news_df['provider_publish_time'] = pd.to_datetime( final_news_df['provider_publish_time'], unit='s')\n",
    "    # Set index to symbol and date\n",
    "    final_news_df.set_index([\"Symbol\", \"provider_publish_time\", \"id\"], inplace=True)\n",
    "    #display(final_news_df.head())\n",
    "    return final_news_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get the data from the existing news csv file in dataframe. New data will be appended to old data\n",
    "This function will open existing csv file and put the data in dataframe and return that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_data(file):\n",
    "    '''\n",
    "    This function open existing news data csv and returns the data in the dataframe\n",
    "    Input Parameters: Name of the existing csv file that needs to be read in dataframe\n",
    "    Returns: pandas dataframe containing data\n",
    "    '''\n",
    "    logging.info(\"Opening existing data file\")\n",
    "    #print(\"Opening existing data file\")\n",
    "    \n",
    "    old_data_df = pd.read_csv(file, encoding='utf-8') # Read the csv file in a dataframe\n",
    "    #display(old_data_df.head())\n",
    "    #old_data_df['provider_publish_time'] = pd.to_datetime(old_data_df['provider_publish_time'], unit='s')\n",
    "    old_data_df['provider_publish_time'] = pd.to_datetime(old_data_df['provider_publish_time'])\n",
    "    #print(type(old_data_df['provider_publish_time']))\n",
    "    old_data_df.set_index([\"Symbol\", \"provider_publish_time\", \"id\"], inplace=True)\n",
    "    #display(old_data_df.head())\n",
    "    return old_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main function to run all the sub functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #tickers_list = ['AAPL', 'fb'] # Created for testing\n",
    "    \n",
    "    # Get todays date in YYYY-MM-DD format\n",
    "    today = pd.Timestamp.now().normalize()\n",
    "    \n",
    "    # Create name of the log file\n",
    "    log_file_name = 'News_yq_logfile_' + str(today).split()[0] +'.log'\n",
    "    #print(f'The log file name is {log_file_name}')\n",
    "    \n",
    "    # Initialize a log file at the Info level. This is just to ensure smooth debugging in case anything fails\n",
    "    # %(asctime)s adds the time of creation of the LogRecord\n",
    "    logging.basicConfig(filename=log_file_name, filemode=\"w\", format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "    \n",
    "    logging.info(\"In the main function\")\n",
    "    logging.info(f'Processing for date {today}')\n",
    "    \n",
    "    # Name of the output file\n",
    "    news_data_file = \"DailyNews_data_yahooquery.csv\"\n",
    "    #print(f'The name of the output file is {news_data_file}')\n",
    "    \n",
    "    logging.info(\"Check if file already exists\")\n",
    "    \n",
    "    # Check if the OHLC csv file existing the root folder from where this script is ran\n",
    "    if os.path.isfile(news_data_file):\n",
    "        logging.info(\"News Data file exists. Pulling only 25 news count\")\n",
    "        #print(\"News Data file exists. Pulling 25 news only \")\n",
    "        # Get the last date and dataframe\n",
    "        old_data_df = get_previous_data(news_data_file)\n",
    "        latest_data = get_latest_news(tickers_list, 25)\n",
    "        # Append new data with old data\n",
    "        final_df = old_data_df.append(latest_data)\n",
    "        # Reset index to remove duplicates\n",
    "        final_df.reset_index(inplace=True)\n",
    "        #print(f'Final dataframe post reset index is {final_df}')\n",
    "        # Drop duplicates that has same symbol and date\n",
    "        final_df = final_df.drop_duplicates(subset=['Symbol','provider_publish_time', 'id'], keep='last')\n",
    "        # Set index to symbol and date\n",
    "        final_df.set_index([\"Symbol\", \"provider_publish_time\", \"id\"], inplace=True)\n",
    "        #print(f'Final dataframe post set index  and sropping duplicates is {final_df}')\n",
    " \n",
    "    else:\n",
    "        logging.info(\"News file does not exists. Getting maximum possible data\")\n",
    "        #print(\"News file does not exists. Getting maximum possible data\")\n",
    "        latest_data = get_latest_news(tickers_list, 500) # Get the max data\n",
    "        final_df = latest_data\n",
    "    \n",
    "    # Create name of the output file\n",
    "    logging.info(f'Writing data in the file name {news_data_file}')\n",
    "    #print(f'Writing data in the file name {news_data_file}')\n",
    "    #display(final_df)\n",
    "    final_df.to_csv(news_data_file, mode='w', index=True, encoding='utf-8') #index is True as we want it to be written in file\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
